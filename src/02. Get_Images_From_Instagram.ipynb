{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Retrieval From Instagram\n",
    "\n",
    "**Goal:** collect image data from instagram and then preprocess it, extract information (image files) from a user's Instagram profile\n",
    "\n",
    "**Constraints:** the user has no way of setting the image size (in KB), the resolution (1080x1080) of the images found on Instagram. The images are extracted from the Instagram page in raw form.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Websites: \n",
    "\n",
    "This notebook's code is based on the following tutorials: \n",
    "\n",
    "https://medium.com/@srujana.rao2/scraping-instagram-with-python-using-selenium-and-beautiful-soup-8b72c186a058\n",
    "\n",
    "https://edmundmartin.com/scraping-instagram-with-python/\n",
    "\n",
    "https://michaeljsanders.com/2017/05/12/scrapin-and-scrollin.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note:** *Remember to respect user‚Äôs rights when you download copyrighted content. Do not use images/videos from Instagram for commercial intent.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import dependencies\n",
    "\n",
    "Install non-standard libraries: requests, BeautifulSoup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from random import choice\n",
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# to install\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build InstagramScraper class\n",
    "based on: https://edmundmartin.com/scraping-instagram-with-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Switching user agents is often a best practice when web scraping and can help you avoid detection. Should the caller of our class have provided their own list of user agents we take a random agent from the provided list.  Otherwise we will return our default user agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a class called InstagramScraper: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url header for requests.get()\n",
    "headers={'User-Agent':  'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36'\n",
    "         ,  'content-type': 'application/json'\n",
    "         , 'accept-encoding': 'gzip, deflate, br'\n",
    "         , 'cache-control': 'no-cache'\n",
    "         , 'accept' : '*/*'\n",
    "         , 'accept-language' : 'de-DE, de; q=0.9,en-US; q=0.8,en;q=0.7'\n",
    "         #, 'referer' : url\n",
    "         , 'connection' : 'keep-alive'\n",
    "         , 'cookie' : 'ig_cb=1; ig_did=DA66C494-9DFE-48F6-BA63-66F11DF8EC03; csrftoken=ukE8jYSjQxVs1YGPYddEkAXsN6WZ4Qmw; mid=XoChrAALAAG78Upva7Ld0TAzeTtm; rur=ASH; urlgen=\"{\\\"2a04:ee41:4:95:91f9:b9d4:8aab:41c\\\": 15796\\054 \\\"213.55.241.7\\\": 15796\\054 \\\"2a04:ee41:4:95:60ae:def3:2fd7:3633\\\": 15796}:1jIpww:PTjjrSzpjC6dWww8-AVOnfdQAFA\"'\n",
    "        }\n",
    "_user_agents = [\n",
    "   'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstagramScraper:\n",
    "\n",
    "    def __init__(self, user_agents=None, proxy=None):\n",
    "        self.user_agents = user_agents\n",
    "        self.proxy = proxy\n",
    "\n",
    "    def __random_agent(self):\n",
    "        if self.user_agents and isinstance(self.user_agents, list):\n",
    "            return choice(self.user_agents)\n",
    "        return choice(_user_agents)\n",
    "\n",
    "    def __request_url(self, url):\n",
    "        \"\"\"Our second helper method is simply a wrapper around requests. \n",
    "        We pass in a URL and try to make a request using the provided user agent and proxy. \n",
    "        If we are unable to make the request or Instagram responds with a non-200 status code we simply re-raise the error. \n",
    "        If everything goes fine, we return the page in questions HTML.\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, headers={'User-Agent': self.__random_agent()}, proxies={'http': self.proxy, 'https': self.proxy})\n",
    "            #response = requests.get(url, headers=headers, proxies={'http': self.proxy, 'https': self.proxy})\n",
    "            response.raise_for_status()\n",
    "        except requests.HTTPError:\n",
    "            raise requests.HTTPError('Received non 200 status code from Instagram')\n",
    "        except requests.RequestException:\n",
    "            raise requests.RequestException('Internet connection failed.')\n",
    "        else:\n",
    "            return response.text\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_json_data(html):\n",
    "        \"\"\"Instagram serve‚Äôs all the of information regarding a user in the form of JavaScript object. \n",
    "        This means that we can extract all of a users profile information and their recent posts by just \n",
    "        making a HTML request to their profile page. We simply need to turn this JavaScript object into \n",
    "        JSON, which is very easy to do.\"\"\"\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        body = soup.find('body')\n",
    "        script_tag = body.find('script')\n",
    "        raw_string = script_tag.text.strip().replace('window._sharedData =', '').replace(';', '')\n",
    "        return json.loads(raw_string)\n",
    "\n",
    "    def profile_page_metrics(self, profile_url):\n",
    "        results = {}\n",
    "        try:\n",
    "            response = self.__request_url(profile_url)\n",
    "            json_data = self.extract_json_data(response)\n",
    "            metrics = json_data['entry_data']['ProfilePage'][0]['graphql']['user']\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        else:\n",
    "            for key, value in metrics.items():\n",
    "                #print('key:', key, '-value:', value)\n",
    "                if key != 'edge_owner_to_timeline_media':\n",
    "                    if value and isinstance(value, dict):\n",
    "                        value = value['count']\n",
    "                        results[key] = value\n",
    "                    elif value:\n",
    "                        results[key] = value\n",
    "        return results\n",
    "\n",
    "    #TODO\n",
    "    def hash_page_metrics(self, profile_url):\n",
    "        results = {}\n",
    "        try:\n",
    "            response = self.__request_url(profile_url)\n",
    "            json_data = self.extract_json_data(response)\n",
    "            metrics = json_data['entry_data']['TagPage'][0]['graphql']['hashtag']\n",
    "         \n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        else:\n",
    "            for key, value in metrics.items():\n",
    "                #print('metrics:', metrics)\n",
    "                if key != 'edge_hashtag_to_media' and key != 'edge_hashtag_to_top_posts' and key != 'profile_pic_url':\n",
    "                    results[key] = value\n",
    "                    if value and isinstance(value, dict):\n",
    "                        try: \n",
    "                            value = value['count']            \n",
    "                            results[key] = value\n",
    "                        except: \n",
    "                            results[key] = value\n",
    "                        try: \n",
    "                            sigma = []\n",
    "                            for i in range(0,5): \n",
    "                                #print(i)\n",
    "                                value = value['edges'][i]['node']['name']  \n",
    "                                #print(i)\n",
    "                            sigma.append(value)\n",
    "                            print(len(value['edges']['node']))\n",
    "                            \n",
    "                            #results[key] = sigma\n",
    "                        except: \n",
    "                            results[key] = value \n",
    "                    elif value:\n",
    "                        results[key] = value\n",
    "        return results\n",
    "    \n",
    "    def profile_page_posts(self, profile_url):\n",
    "        results = []\n",
    "        try:\n",
    "            response = self.__request_url(profile_url)\n",
    "            json_data = self.extract_json_data(response)\n",
    "            metrics = json_data['entry_data']['ProfilePage'][0]['graphql']['user']['edge_owner_to_timeline_media'][\"edges\"]\n",
    "            #pprint(metrics)\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        else:\n",
    "            for node in metrics:\n",
    "                node = node.get('node')\n",
    "                #if node and isinstance(node, dict): #this line only gets most recent post out\n",
    "                results.append(node)\n",
    "        return results\n",
    "    \n",
    "    def hashtag_page_posts(self, hashtag_url):\n",
    "        results = []\n",
    "        try:\n",
    "            response = self.__request_url(hashtag_url)\n",
    "            json_data = self.extract_json_data(response)\n",
    "            #pprint(json_data)\n",
    "            metrics = json_data['entry_data']['TagPage'][0]['graphql']['hashtag']['edge_hashtag_to_media'][\"edges\"]\n",
    "            #pprint(metrics)\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        else:\n",
    "            for node in metrics:\n",
    "                node = node.get('node')\n",
    "                #if node and isinstance(node, dict): #this line only gets most recent post out\n",
    "                results.append(node)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Load URLS of Brand Names Data\n",
    "\n",
    "Specify instragram USERNAME profile whose page you want to scrape. Get a dictionary with all information (image, comments, etc.) from that Instagram profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to specify\n",
    "directory= r'C:\\Users\\Anonym\\Documents\\GitHub\\DLfM_BrandManagement\\data\\instagram_urls'\n",
    "os.chdir(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get out all apparel brands. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abercrombie', 'adidas', 'anntaylor', 'bacardiusa', 'bananarepublic']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "data = pd.read_csv(\"firm_usernames.csv\", header=None)\n",
    "\n",
    "firm_usernames = data[0].tolist()\n",
    "firm_usernames[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fanta', 'coorslight', 'greygoose', 'corona', 'monsterenergy']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"instagram_hashtags.csv\", header=None)\n",
    "\n",
    "instagram_hashtags = data[0].tolist()\n",
    "instagram_hashtags[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform set theory on both datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2): \n",
    "    lst3 = [value for value in lst1 if value in lst2] \n",
    "    return lst3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_intersection(lst1, lst2): \n",
    "    lst3 = list(set(lst1) ^ set(lst2))\n",
    "    return lst3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_difference(lst1, lst2): \n",
    "    lst3 = list(set(lst1) - set(lst2))\n",
    "    return lst3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of brands firm usernames:  45\n",
      "Number of brands as instagram hashtags:  56\n",
      "Number of same brands (firm usernames and hashtags):  36\n",
      "Brands that are both firm usernames and hashtags:  ['abercrombie', 'adidas', 'anntaylor', 'bananarepublic', 'carhartt'] ...\n",
      "Brands that are in neither firm usernames nor hashtags:  ['jackdaniels', 'budlight', 'sanpellegrino', 'dockerskhakis', 'hanesbrasil']\n",
      "Brands that are firm usernames only:  ['hanesbrasil', 'nesquikusa', 'sanpellegrino_official', 'hollisterco', 'joeboxerlicky']\n",
      "Brands that are hashtagged only:  ['jackdaniels', 'budlight', 'sanpellegrino', 'hollister', 'bacardi']\n"
     ]
    }
   ],
   "source": [
    "print('Number of brands firm usernames: ', len(firm_usernames))\n",
    "print('Number of brands as instagram hashtags: ', len(instagram_hashtags))\n",
    "print('Number of same brands (firm usernames and hashtags): ', len(intersection(firm_usernames, instagram_hashtags)))\n",
    "print('Brands that are both firm usernames and hashtags: ', intersection(firm_usernames, instagram_hashtags)[:5], '...')\n",
    "print('Brands that are in neither firm usernames nor hashtags: ', non_intersection(firm_usernames, instagram_hashtags)[:5])\n",
    "print('Brands that are firm usernames only: ', set_difference(firm_usernames, instagram_hashtags)[:5])\n",
    "print('Brands that are hashtagged only: ', set_difference(instagram_hashtags, firm_usernames)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Specify Instagram page(s)\n",
    "\n",
    "Specify instragram USERNAME profile whose page you want to scrape. Get a dictionary with all information (image, comments, etc.) from that Instagram profile. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User-profile Page\n",
    "\n",
    "If you want to scrape a user-profile page, specify the username as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pages that have access denial because of age limit\n",
    "# are you 18/21 or over? \n",
    "#urls.remove('https://www.instagram.com/bacardiusa/?hl=en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# items to be removed from list\n",
    "agelimited_brands = {'bacardiusa', 'budlight', 'budweiser', 'coorslight', 'corona', 'greygoose', 'jackdaniels_us', 'korbel_1882'} \n",
    "  \n",
    "firm_usernames = [ele for ele in firm_usernames if ele not in agelimited_brands] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.instagram.com/abercrombie/?hl=en',\n",
       " 'https://www.instagram.com/adidas/?hl=en']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for multiple firms  \n",
    "urls = []\n",
    "hashtag = False\n",
    "\n",
    "for username in firm_usernames: \n",
    "    url = 'https://www.instagram.com/'+username+'/?hl=en'\n",
    "    urls.append(url)\n",
    "\n",
    "urls[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for one firm only \n",
    "\n",
    "# to specify\n",
    "#username='cailler_suisse'\n",
    "#hashtag = False\n",
    "#url = 'https://www.instagram.com/'+username+'/?hl=en'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashtag Page\n",
    "\n",
    "If you want to open a hashtag page (instead of a user profile): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.instagram.com/explore/tags/fanta',\n",
       " 'https://www.instagram.com/explore/tags/coorslight']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for multiple brands  \n",
    "hash_urls = []\n",
    "username = False\n",
    "\n",
    "for hashtag in instagram_hashtags: \n",
    "    url = 'https://www.instagram.com/explore/tags/'+hashtag\n",
    "    hash_urls.append(url)\n",
    "\n",
    "hash_urls[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for one brand only \n",
    "\n",
    "# to specify\n",
    "#hashtag='cailler'\n",
    "#username = False\n",
    "#url = 'https://www.instagram.com/explore/tags/'+hashtag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Get information from Instagram page(s) [optional]\n",
    "\n",
    "Now that the url of the Instagram page is defined, it will extract out all the posts or meta-information from the website usinge the InstagramScraper class. \n",
    "\n",
    "Get meta-information metrics by using a class method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'biography': 'üå±plant-based recipes & wholesome living \\n'\n",
      "              'üçínourish the cells & the soul \\n'\n",
      "              'üå±a YouTube community of 2M friends üë©üèª\\u200düåæ\\n'\n",
      "              'üëá NEW VIDEO üëá',\n",
      " 'business_category_name': 'Publishers',\n",
      " 'category_id': '2707',\n",
      " 'edge_felix_video_timeline': 0,\n",
      " 'edge_follow': 127,\n",
      " 'edge_followed_by': 531071,\n",
      " 'edge_media_collections': 0,\n",
      " 'edge_mutual_followed_by': 0,\n",
      " 'edge_saved_media': 0,\n",
      " 'external_url': 'https://youtu.be/0Kgi-H2W7Hk',\n",
      " 'external_url_linkshimmed': 'https://l.instagram.com/?u=https%3A%2F%2Fyoutu.be%2F0Kgi-H2W7Hk&e=ATM5rZNI8I5aBiZz3RAszJWMkhflagAU_QiH_SQDII3ITWclaigcQbJHAT__clKn0V1x15eE&s=1',\n",
      " 'full_name': 'Sadia Badiei, BSc Dietetics',\n",
      " 'highlight_reel_count': 1,\n",
      " 'id': '2072931271',\n",
      " 'is_business_account': True,\n",
      " 'is_verified': True,\n",
      " 'profile_pic_url': 'https://instagram.fzrh2-1.fna.fbcdn.net/v/t51.2885-19/s150x150/84057956_823380854858266_527460638654464000_n.jpg?_nc_ht=instagram.fzrh2-1.fna.fbcdn.net&_nc_ohc=RvJ85_MOJB4AX_BHBpW&oh=b186936487509345806919d712d1c3fd&oe=5EAC5272',\n",
      " 'profile_pic_url_hd': 'https://instagram.fzrh2-1.fna.fbcdn.net/v/t51.2885-19/s320x320/84057956_823380854858266_527460638654464000_n.jpg?_nc_ht=instagram.fzrh2-1.fna.fbcdn.net&_nc_ohc=RvJ85_MOJB4AX_BHBpW&oh=73434471df87cec9e259b96c706cca41&oe=5EA954FD',\n",
      " 'username': 'pickuplimes'}\n"
     ]
    }
   ],
   "source": [
    "# get profile page metrics\n",
    "from pprint import pprint\n",
    "\n",
    "k = InstagramScraper()\n",
    "results = k.profile_page_metrics(url) \n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get hashtag page metrics\n",
    "from pprint import pprint\n",
    "\n",
    "k = InstagramScraper()\n",
    "#TODO\n",
    "results = k.hash_page_metrics(url) \n",
    "#pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Get image posts from Instagram page(s)\n",
    "\n",
    "Get all posts on an Instagram **profile page** that are visible on the landing page (more items only load as you scroll downwards). The page loads 12 items at a time, and I need to scroll to load all entries (for a total of 120)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User-profile Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Instagram pages:  45\n",
      "Total number of images:  540\n",
      "Average number of images per Instagram hashtag page:  12.0\n"
     ]
    }
   ],
   "source": [
    "# get posts (images) from multiple profile pages \n",
    "from pprint import pprint\n",
    "\n",
    "resultz = []\n",
    "for url in urls: \n",
    "    k = InstagramScraper()\n",
    "    results = k.profile_page_posts(url)\n",
    "    resultz.append(results)\n",
    "    print('Instagram page: ', url)\n",
    "\n",
    "print('Total number of Instagram user-profile pages: ', len(resultz))\n",
    "print('Total number of images: ', len(resultz)*len(resultz[0]))\n",
    "print('Average number of images per Instagram user-profile page: ', len(resultz)*len(resultz[0])/len(resultz) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get posts (images) from single profile page \n",
    "\n",
    "#from pprint import pprint\n",
    "\n",
    "#k = InstagramScraper()\n",
    "#results = k.profile_page_posts(url)\n",
    "\n",
    "#print('Instagram page: ', url)\n",
    "#print('Posts on Instagram profile page: ', len(results))\n",
    "#print('Second image url on instagram profile: ', results[1]['display_url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashtag Page\n",
    "\n",
    "Get all posts on an Instagram **hashtag page** that are visible on the landing page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Instagram hashtag pages:  56\n",
      "Total number of hashed images:  3976\n",
      "Average number of images per Instagram hashtag page:  71.0\n"
     ]
    }
   ],
   "source": [
    "# get posts (images) from multiple hashtag pages \n",
    "from pprint import pprint\n",
    "\n",
    "hash_result = []\n",
    "for url in hash_urls: \n",
    "    k = InstagramScraper()\n",
    "    results = k.hashtag_page_posts(url)\n",
    "    hash_result.append(results)\n",
    "    #print('Instagram page: ', url)\n",
    "\n",
    "print('Total number of Instagram hashtag pages: ', len(hash_result))\n",
    "print('Total number of hashed images: ', len(hash_result)*len(hash_result[0]))\n",
    "print('Average number of images per Instagram hashtag page: ', len(hash_result)*len(hash_result[0])/len(hash_result) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second image url on instagram hashtag:  https://instagram.fzrh2-1.fna.fbcdn.net/v/t51.2885-15/e35/91239097_142996663896683_8540617356821851109_n.jpg?_nc_ht=instagram.fzrh2-1.fna.fbcdn.net&_nc_cat=111&_nc_ohc=A929RCiLgugAX-gB3d_&oh=1518ce05779916dbee5bb470c34acaf5&oe=5EAE7D88\n"
     ]
    }
   ],
   "source": [
    "# get posts (images) from a hashtag page \n",
    "#from pprint import pprint\n",
    "\n",
    "#k = InstagramScraper()\n",
    "#results = k.hashtag_page_posts(url)\n",
    "\n",
    "#pprint(results)\n",
    "#print('Instagram page: ', url)\n",
    "#print('Posts on Instagram hashtag page: ', len(results))\n",
    "#print('Second image url on instagram hashtag: ', results[1]['display_url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Save images from list of dict \n",
    "\n",
    "Use requests library to download images from the ‚Äòdisplay_url‚Äô in pandas ‚Äòresult‚Äô data frame and store them with respective shortcode as file name.\n",
    "\n",
    "Specify the directory for storing the images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load modules\n",
    "import os\n",
    "import requests\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some functions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_root_path_images(): \n",
    "    \n",
    "    # to specify\n",
    "    directory= r\"C:\\Users\\Anonym\\Documents\\GitHub\\DLfM_BrandManagement\\data\"\n",
    "    folder = 'instagram_images' #image root folder, all subfolders' name are firmnames\n",
    "\n",
    "    os.chdir(directory)\n",
    "\n",
    "    try: \n",
    "        os.mkdir(folder)\n",
    "    except: \n",
    "        pass\n",
    "\n",
    "    path = os.path.join(directory, folder)\n",
    "    os.chdir(path)\n",
    "    return path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_folders_images(account, folder, path): \n",
    "        try: \n",
    "            os.mkdir(os.path.join(path, account))\n",
    "        except: \n",
    "            pass\n",
    "        \n",
    "        # set directory \n",
    "        directory = os.path.join(path, account)\n",
    "        os.chdir(directory)   \n",
    "        try: \n",
    "            os.mkdir(folder)\n",
    "            print('new folder created for: ', account)\n",
    "        except: \n",
    "            pass\n",
    "        path = os.path.join(directory, folder)\n",
    "        os.chdir(path)\n",
    "        return path \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### User-profile page\n",
    " \n",
    " Save all images from user-profile Instagram pages to your folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download all visible images from multiple Instagram pages \n",
    "\n",
    "for i, username in enumerate(firm_usernames): \n",
    "    path = set_root_path_images()\n",
    "    build_folders_images(username, 'user_profile', path)\n",
    "\n",
    "    # get image url \n",
    "    for j in range(len(resultz[i])): \n",
    "        r = requests.get(resultz[i][j]['display_url'], stream=True)\n",
    "        with open(resultz[i][j]['shortcode']+\".jpg\", 'wb') as f:\n",
    "            # Set decode_content value to True, otherwise the downloaded image file's size will be zero.\n",
    "            r.raw.decode_content = True\n",
    "            # Copy the response stream raw data to local image file.\n",
    "            shutil.copyfileobj(r.raw, f)\n",
    "            # Remove the image url response object.\n",
    "            del r\n",
    "            \n",
    "    print('processed: ', username, ' .', i, ' out of ', len(firm_usernames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Hashtag page\n",
    " \n",
    " Save all images from hashtag Instagram pages to your folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download all visible images from multiple Instagram pages \n",
    "\n",
    "for i, hashtag in enumerate(instagram_hashtags):\n",
    "    path = set_root_path_images()\n",
    "    build_folders_images(hashtag, 'hashtag', path)\n",
    "\n",
    "    # get image url \n",
    "    for j in range(len(hash_result[i])): \n",
    "        r = requests.get(hash_result[i][j]['display_url'], stream=True)\n",
    "        with open(hash_result[i][j]['shortcode']+\".jpg\", 'wb') as f:\n",
    "            # Set decode_content value to True, otherwise the downloaded image file's size will be zero.\n",
    "            r.raw.decode_content = True\n",
    "            # Copy the response stream raw data to local image file.\n",
    "            shutil.copyfileobj(r.raw, f)\n",
    "            # Remove the image url response object.\n",
    "            del r\n",
    "    print('processed: ', hashtag, ' .', i, ' out of ', len(instagram_hashtags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Both pages\n",
    " \n",
    " Save all images from both user profile and hashtag Instagram pages to your folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download all visible images from multiple Instagram pages \n",
    "\n",
    "if firm_usernames: \n",
    "    for i, username in enumerate(firm_usernames): \n",
    "        path = set_root_path_images()\n",
    "        build_folders_images(username, 'user_profile', path)\n",
    "       \n",
    "        # get image url \n",
    "        for j in range(len(resultz[i])): \n",
    "            r = requests.get(resultz[i][j]['display_url'], stream=True)\n",
    "            with open(resultz[i][j]['shortcode']+\".jpg\", 'wb') as f:\n",
    "                # Set decode_content value to True, otherwise the downloaded image file's size will be zero.\n",
    "                r.raw.decode_content = True\n",
    "                # Copy the response stream raw data to local image file.\n",
    "                shutil.copyfileobj(r.raw, f)\n",
    "                # Remove the image url response object.\n",
    "                del r\n",
    "\n",
    "elif instagram_hashtags: \n",
    "    for i, hashtag in enumerate(instagram_hashtags):\n",
    "        path = set_root_path_images()\n",
    "        build_folders_images(hashtag, 'hashtag', path)\n",
    "\n",
    "        # get image url \n",
    "        for j in range(len(hash_result[i])): \n",
    "            r = requests.get(hash_result[i][j]['display_url'], stream=True)\n",
    "            with open(hash_result[i][j]['shortcode']+\".jpg\", 'wb') as f:\n",
    "                # Set decode_content value to True, otherwise the downloaded image file's size will be zero.\n",
    "                r.raw.decode_content = True\n",
    "                # Copy the response stream raw data to local image file.\n",
    "                shutil.copyfileobj(r.raw, f)\n",
    "                # Remove the image url response object.\n",
    "                del r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download all visible images from an Instagram page \n",
    "\n",
    "path = set_root_path_images()\n",
    "\n",
    "if username: \n",
    "    build_folders_images(username, 'user_profile', path)\n",
    "elif hashtag: \n",
    "    build_folders_images(hashtag, 'hashtag', path)\n",
    "\n",
    "for i in range(len(results)):\n",
    "    r = requests.get(results[i]['display_url'], stream=True)\n",
    "    with open(results[i]['shortcode']+\".jpg\", 'wb') as f:\n",
    "        # Set decode_content value to True, otherwise the downloaded image file's size will be zero.\n",
    "        r.raw.decode_content = True\n",
    "        # Copy the response stream raw data to local image file.\n",
    "        shutil.copyfileobj(r.raw, f)\n",
    "        # Remove the image url response object.\n",
    "        del r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download one image only\n",
    "\n",
    "path = set_root_path_images()\n",
    "\n",
    "r = requests.get(url, stream=True)\n",
    "\n",
    "with open(directory+\"B-Tckr0AgrH\"+\".jpg\", 'wb') as f:\n",
    "    # Set decode_content value to True, otherwise the downloaded image file's size will be zero.\n",
    "    r.raw.decode_content = True\n",
    "    # Copy the response stream raw data to local image file.\n",
    "    shutil.copyfileobj(r.raw, f)\n",
    "    # Remove the image url response object.\n",
    "    del r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
